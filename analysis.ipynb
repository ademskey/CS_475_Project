{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068da808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Analysis section of the project. This section will consist of \n",
    "# exploratory data analysis (EDA) and visualization of the datasets used in the project.\n",
    "# These visualizations will help to understand the data and provide insights that may be useful for \n",
    "# future analysis and training of ML models.ipynb\n",
    "\n",
    "# Sources:\n",
    "# Disclaimer: GenAI was used for idea generation, suggestions, and debugging but not for full code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55f9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from matplotlib.pyplot import subplots\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517e130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets with pandas\n",
    "filepath = os.getcwd() + \"/datasets\"\n",
    "dataset_4_1 = pd.read_csv(os.path.join(filepath, \"4.1_final_assignments.csv\"))\n",
    "dataset_4_2 = pd.read_csv(os.path.join(filepath, \"4.2_final_assignments.csv\"))\n",
    "dataset_4_3 = pd.read_csv(os.path.join(filepath, \"4.3_final_assignments.csv\"))\n",
    "dataset_4_4 = pd.read_csv(os.path.join(filepath, \"4.4_final_assignments.csv\"))\n",
    "dataset_4_5 = pd.read_csv(os.path.join(filepath, \"4.5_final_assignments.csv\"))\n",
    "dataset_8_1 = pd.read_csv(os.path.join(filepath, \"8.1_final_assignments.csv\"))\n",
    "dataset_8_2 = pd.read_csv(os.path.join(filepath, \"8.2_final_assignments.csv\"))\n",
    "dataset_8_3 = pd.read_csv(os.path.join(filepath, \"8.3_final_assignments.csv\"))\n",
    "dataset_8_4 = pd.read_csv(os.path.join(filepath, \"8.4_final_assignments.csv\"))\n",
    "dataset_8_5 = pd.read_csv(os.path.join(filepath, \"8.5_final_assignments.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f535f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4m/g79cc05d4jg252d8xkyzdbfr0000gn/T/ipykernel_16919/2739461662.py:13: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  combined_dataset['collection.date'] = pd.to_datetime(combined_dataset['collection.date'], errors='ignore')\n",
      "/var/folders/4m/g79cc05d4jg252d8xkyzdbfr0000gn/T/ipykernel_16919/2739461662.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  combined_dataset['collection.date'] = pd.to_datetime(combined_dataset['collection.date'], errors='ignore')\n",
      "/var/folders/4m/g79cc05d4jg252d8xkyzdbfr0000gn/T/ipykernel_16919/2739461662.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_dataset['Size (Kb)'].fillna(median_size, inplace=True)\n",
      "/var/folders/4m/g79cc05d4jg252d8xkyzdbfr0000gn/T/ipykernel_16919/2739461662.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_dataset[col].fillna('unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Combine Datasets into one set for visualization and analysis\n",
    "combined_dataset = pd.concat([dataset_4_1, dataset_4_2, dataset_4_3, dataset_4_4, dataset_4_5, dataset_8_1, \n",
    "                              dataset_8_2, dataset_8_3, dataset_8_4, dataset_8_5], ignore_index=True)\n",
    "\n",
    "# Drop any fully duplicated rows\n",
    "combined_dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop rows where critical values are missing (essentially dropNA on key columns)\n",
    "critical_cols = ['wwtp', 'wrf', 'GenomeName', 'Proteins', 'network']\n",
    "combined_dataset.dropna(subset=critical_cols, inplace=True)\n",
    "\n",
    "# Convert time and pore size collumns to appropriate data types\n",
    "combined_dataset['collection.date'] = pd.to_datetime(combined_dataset['collection.date'], errors='ignore')\n",
    "combined_dataset['pore.size.um'] = combined_dataset['pore.size'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "combined_dataset.drop('pore.size', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Standardize string columns and clean prediction columns\n",
    "object_cols = combined_dataset.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "for col in object_cols:\n",
    "    combined_dataset[col] = combined_dataset[col].astype(str).str.lower()\n",
    "    \n",
    "    # Clean predictions (e.x 'taleaviricota|nan' -> 'taleaviricota')\n",
    "    if 'prediction' in col:\n",
    "        # Split by '|' and take the first element (the most likely prediction)\n",
    "        combined_dataset[col] = combined_dataset[col].str.split('|').str[0]\n",
    "\n",
    "# Fill nan Genome Size (Kb) with the median\n",
    "median_size = combined_dataset['Size (Kb)'].median()\n",
    "combined_dataset['Size (Kb)'].fillna(median_size, inplace=True)\n",
    "\n",
    "# Fill remaining NaN values in object columns with 'unknown'\n",
    "object_cols_for_filling = combined_dataset.select_dtypes(include='object').columns.tolist()\n",
    "for col in object_cols_for_filling:\n",
    "    combined_dataset[col].fillna('unknown', inplace=True)\n",
    "\n",
    "# Generate CSV of combined dataset for future use\n",
    "combined_dataset.to_csv(os.path.join(filepath, \"combined_final_assignments.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a20ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
